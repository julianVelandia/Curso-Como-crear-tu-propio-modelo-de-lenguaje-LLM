{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3690230b",
   "metadata": {
    "id": "3690230b"
   },
   "source": [
    "# Finetuning Llama 3 con LoRA\n",
    "\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde1344",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbde1344",
    "outputId": "cfc66d6f-f191-472e-dfcd-d4067fc310e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.8.0+cu126\n",
      "Uninstalling torch-2.8.0+cu126:\n",
      "  Successfully uninstalled torch-2.8.0+cu126\n",
      "Found existing installation: torchvision 0.23.0+cu126\n",
      "Uninstalling torchvision-0.23.0+cu126:\n",
      "  Successfully uninstalled torchvision-0.23.0+cu126\n",
      "Found existing installation: torchaudio 2.8.0+cu126\n",
      "Uninstalling torchaudio-2.8.0+cu126:\n",
      "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.6.0+cu124\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.21.0+cu124\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.6.0+cu124\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0+cu124) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0+cu124) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0+cu124) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0+cu124) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0+cu124) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m266.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m180.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m181.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m193.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m172.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m179.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m187.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0+cu124) (75.2.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0+cu124)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m300.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0+cu124) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0+cu124) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl (768.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m289.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m235.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m291.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m307.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m233.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch torchvision torchaudio\n",
    "!pip install --no-cache-dir torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k-11a5VBb8s6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-11a5VBb8s6",
    "outputId": "6ad5327a-9b61-43af-d212-cf13a3a95685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cu124\n",
      "transformers: 4.55.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98334c",
   "metadata": {
    "id": "8a98334c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdea7d",
   "metadata": {
    "id": "6efdea7d"
   },
   "outputs": [],
   "source": [
    "\n",
    "DATASET_NAME = \"JulianVelandia/ColombianAccent\"\n",
    "MODEL_NAME   = \"meta-llama/Llama-3.2-1B\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/modelos-edteam/Llama-3.2-1B-acento-ft\"\n",
    "HUGGINGFACE_TOKEN = \":)\"\n",
    "LORA_RANK    = 32     # cuántos parámetros extra entrena LoRA\n",
    "LORA_ALPHA   = 64     # qué tanto pesan esas actualizaciones\n",
    "LORA_DROPOUT = 0.05   # chance de apagar conexiones para no sobreajustar\n",
    "\n",
    "LEARNING_RATE         = 1e-4  # qué tan grandes son los pasos de aprendizaje\n",
    "PER_DEVICE_BATCH_SIZE = 2     # cuántos ejemplos procesa a la vez por GPU/CPU\n",
    "GRADIENT_ACCUMULATION = 4     # acumula pasos chicos para simular un batch grande\n",
    "EPOCHS                = 3     # veces que recorre todo el dataset\n",
    "SAVE_STEPS            = 250   # cada cuántos pasos guarda un checkpoint\n",
    "\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1PeSJxEm8fQq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PeSJxEm8fQq",
    "outputId": "a441d038-f727-4315-c56a-38cfe06d563b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322c5a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "247cf8c3a7f34cc9a05ac189f3df04f0",
      "6edd23e416e541198ac12054ca1e0337",
      "1c7cdac8704e4291908af9fa0da13157",
      "24f4ce4287db4645bc6604a3823f0219",
      "2dae0858c51843eab6fc11dd92ab84fa",
      "51ef535518614008998ba15ae952a879",
      "5ea4ad831ef8472a9d2b983fa37499a5",
      "747bac85daff45ff8fb75aa17713ddd2",
      "662405cf1af64e3c81a40de5f46958fa",
      "0d002b8a4a21407f97ad2a2a51a37ef2",
      "9ac90eac1d04493d911037b025ebc10f",
      "f8d52aa8440b45a6b19bd61f2ff6e617",
      "17afe46f123f476ebb446fa58c90f1ec",
      "e67c63ad01f741408a7707b508f3d678",
      "853bfef7696f4f169278d041c2c815d4",
      "dd33c9213b024ff8b44cacc47f99a683",
      "f642abc23c6841eb8cb9e0d77acc65ff",
      "e494aa9163b14b19a6269451a3b89f0a",
      "efe25e7749e043ad96a01ee5336b7967",
      "fbdabf4daa5b4c6394e6c3fb72adcdb9",
      "25a1e3792d684e4abb276bc5a1fc77a0",
      "7a67eb7007814adebab22ce6d6e95fa2",
      "09bd67ca43ab4e6bba192b9b8b8d497c",
      "0cdde993517140e2bb3a0abe233323a5",
      "1b0fa2ce8a854104a9d03d388b2b0f84",
      "d0958108777b48549ae59a69e3a01b67",
      "7a7c2e2f461f4e90be1b2779c708cb4b",
      "f99d4119d8df4e1aaab08fee06df99e6",
      "1c2de850aa864a0aaac4535a622f429a",
      "34a2d0b9eca64423bf364252810e1b50",
      "8705294de5994805a58a0c11836f768f",
      "074ca1a8c0a2429ab94c183f22acced1",
      "3a8953ee098345bcbed1dd38b28643da"
     ]
    },
    "id": "d322c5a8",
    "outputId": "599afe94-a63c-4a44-968f-e470047c0cdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247cf8c3a7f34cc9a05ac189f3df04f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d52aa8440b45a6b19bd61f2ff6e617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/6.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bd67ca43ab4e6bba192b9b8b8d497c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/30833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas: ['dataset', 'instruction', 'response']\n",
      "{'dataset': 'Nicky0007/cointelegraph_noticias_Es', 'instruction': '¿Qué pasa, hermano, con el optimismo sobre Bitcoin después del abril loco en las criptomonedas?', 'response': 'Pues, parce, parece que el precio de Bitcoin está bajando y los inversores están un poco desanimados.'}\n"
     ]
    }
   ],
   "source": [
    "#Cargar dataset\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(\"Columnas:\", dataset.column_names)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580a36c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318,
     "referenced_widgets": [
      "6c7d0e331c694d3da5be4d53905f476c",
      "d05d519510a648deb7dd9038e4d09fd1",
      "39a9efbd442549ed936b11867a1a0e5c",
      "ee36b33c81ec47b5aae54bce0ccf6cd2",
      "5570c34a6e9b49c59645b6d2a3c82a15",
      "cfaaafa58e7d43419ad63dfc48e8ca50",
      "236c5d32c9ae45a284e8f3a384911ab6",
      "b0b29c0aa2b74464b7355e9fafd44749",
      "f44cc428cc78476186f8b757a929629c",
      "e7d3270fc4754ed5a84d190afb6be989",
      "915f5404edcd4b198eaea00252bf0e00",
      "4cb43cd186af48668c7183585bdd64d2",
      "383a5bf8d4844bcba12d34157fc176cf",
      "f7860202c9e64c04909ae98ffa718695",
      "9b2309882b0241489d58bebc4a670e2c",
      "6d45a4f0cd5a4567953e8f053c597919",
      "67b64c445faf4452ae0a942450e53b65",
      "95a77c63c88343898148b66bdd669ce3",
      "9270c352f8b041ffaa979a8358050148",
      "b96909e2f6b3460f893768497eed9d61",
      "b492937c4b294f43b99389855d460292",
      "11196324d68744329f665daed7a00b65",
      "c3545203fead40e1866c9bfbc6ef7aae",
      "3b496feb9d39439e96a624c8bbe7268c",
      "253d4898284947238cae297620e93958",
      "40dfdc61b2cf4c33b9957ec8ab08c87b",
      "2172c457df3543fa9a0aa61d43bf271f",
      "1a13257237da4f1f9f599d219379ab2e",
      "19d050b4f5e54c40822f9b598f0a2090",
      "23d0416b727f402e81385f98f23369ed",
      "a73024f16aae4225a7affff8fe8dcb09",
      "54cb21a9d64f444b8065be52ff7560c6",
      "359e7d6c2c504c1594f5c00d22055eed",
      "5efe54012de74679bdb15b80597b137e",
      "cb953b439d744bd28b51922b184ce043",
      "c524d637b85d4a4b946696948bacc526",
      "01853ec83df64cb18579f1b6bcc595d5",
      "16fc0e414f50435db7a37ee239feeaf6",
      "9338a85b3ac146c28b495bd53f380395",
      "5c2ae09809374b66ba22a3eaa8aad1ac",
      "00133fe26b354f35937d6811ddf22b1e",
      "f3f5543a4fc1447d85941fdf39f5f314",
      "a7c1b69c40384e569b0262d6f15a5e22",
      "8ad8ea99ed3d4d1abf7cebe02425406b",
      "8b273705d71645978ecd75875aafca8f",
      "114e23f1c13549d6b85f6f2a20beda79",
      "fdc5dd97f61046dfbd9ad39ff554b7de",
      "3484f6c9891c4fa986c22aa1bede495f",
      "dd5679abd3f849cca16983b1d22206d4",
      "e0aa7fa1a1e64cd0ad1c40cd69907478",
      "c2ec39272f264520b1e52ba04416bfa9",
      "c0edd88f34714487a9452996e53b7c38",
      "70f77f0622f645049e31efbc3ad07356",
      "c01515e11df1440b988341cdd2f64c6f",
      "81417ea95efe453f90c837656cbe7529",
      "568b966ed4dc4bcda3ff4f4565f02f86",
      "12580792215745a59ef0943904e227d2",
      "76af89d93a874eed908927b5ac250ac0",
      "2e741391df2545d1bfda53550ff7f0b8",
      "ed79c0efaf4e4851b27b871814634a46",
      "941c973868ce4017ae07010017ce802f",
      "bfeebf892a04456fab7765e2882e1f84",
      "1c4555a6f8c14893bc363e84f7158535",
      "fc308d9713254b21af21b7e7f237da60",
      "21d5552ab7904b7786ba7fe144aaa18a",
      "a302002fdc99458abed78e4bc53ecadf"
     ]
    },
    "id": "8580a36c",
    "outputId": "d94d7d6e-c605-42c7-9648-a664b30e562a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7d0e331c694d3da5be4d53905f476c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb43cd186af48668c7183585bdd64d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3545203fead40e1866c9bfbc6ef7aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1001: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efe54012de74679bdb15b80597b137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b273705d71645978ecd75875aafca8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568b966ed4dc4bcda3ff4f4565f02f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128256, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar modelo y tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if loop.first %}{{ bos_token }}{% endif %}<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>{% endfor %}{% if add_generation_prompt %}<|start_header_id|>assistant<|end_header_id|>\\n\\n{% endif %}\"\n",
    "\n",
    "#pad_token rellena los huecos y deja todas las secuencias del mismo tamaño\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac858f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eac858f6",
    "outputId": "b77cf8cf-79e4-4f2d-a03d-9f65a41f4ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n"
     ]
    }
   ],
   "source": [
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd569ea0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "ee04547701a64d27b4964e5223a85dc1",
      "13e9cf204574462aa241886f274ea9f2",
      "3dda6074245b4c90b98a7419c4c0cd5f",
      "cda48bdaa951462a9987cf0361f6c420",
      "8fe47733c446486d839887a4c564bdf6",
      "d05368e9ba614d9ba16ae423c9dcd338",
      "d0fc1edbe1c24542ace47c29f8eca5c3",
      "e268ba7d9238458bb46620e92a6d1e5e",
      "210b2eeb7a4f461689bb536fc6dd7702",
      "a48ee4282a714a5d91ab9d2da368f4ae",
      "145c87c4001d421dbba30ebaf19782a6"
     ]
    },
    "id": "fd569ea0",
    "outputId": "fa505a99-07b8-4417-d018-774f8f04dbb0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee04547701a64d27b4964e5223a85dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenizado y listo\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 1024\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    msgs_prompt = [[{\"role\":\"user\",\"content\":u}] for u in examples[\"instruction\"]]\n",
    "    msgs_full   = [[{\"role\":\"user\",\"content\":u},{\"role\":\"assistant\",\"content\":r}] for u,r in zip(examples[\"instruction\"], examples[\"response\"])]\n",
    "\n",
    "    prompt_txt = [tokenizer.apply_chat_template(ms, tokenize=False, add_generation_prompt=True) for ms in msgs_prompt]\n",
    "    full_txt   = [tokenizer.apply_chat_template(ms, tokenize=False, add_generation_prompt=False) for ms in msgs_full]\n",
    "\n",
    "    tok_prompt = tokenizer(prompt_txt, add_special_tokens=False, truncation=True, max_length=MAX_LEN)\n",
    "    tok_full   = tokenizer(full_txt, add_special_tokens=False, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "    input_ids = tok_full[\"input_ids\"]\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    labels = []\n",
    "    for ids, p_ids in zip(input_ids, tok_prompt[\"input_ids\"]):\n",
    "        n_prompt = len(p_ids)\n",
    "        lab = ids.copy()\n",
    "        for j in range(len(lab)):\n",
    "            if j < n_prompt or lab[j] == pad_id:\n",
    "                lab[j] = -100\n",
    "        labels.append(lab)\n",
    "\n",
    "    tok_full[\"labels\"] = labels\n",
    "    return tok_full\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "print(\"Dataset tokenizado y listo\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"no\",# no evalúa durante el entrenamiento\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2, # guarda solo los últimos 2 checkpoints\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION, # junta pasos chicos para simular batch grande\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    fp16=True, # usa media precisión para ahorrar memoria y acelerar\n",
    "    push_to_hub=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68abd17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d68abd17",
    "outputId": "632ab80e-303d-418b-beb4-4d44c9602da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reanudando desde /content/drive/MyDrive/modelos-edteam/Llama-3.2-1B-acento-ft/checkpoint-7500\n"
     ]
    }
   ],
   "source": [
    "# Checkpoints previos\n",
    "import re\n",
    "\n",
    "last_checkpoint = None\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    checkpoints = [\n",
    "        d for d in os.listdir(OUTPUT_DIR)\n",
    "        if re.match(r\"^checkpoint-\\d+$\", d)\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = os.path.join(\n",
    "            OUTPUT_DIR,\n",
    "            max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "        )\n",
    "\n",
    "if last_checkpoint:\n",
    "    print(f\"Reanudando desde {last_checkpoint}\")\n",
    "else:\n",
    "    print(\"Sin checkpoint previo\")\n",
    "\n",
    "\n",
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Checkpoint guardado en paso {state.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d13f2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 948
    },
    "id": "13d13f2e",
    "outputId": "8d58fd98-bf9a-4511-dcf9-5eb8cb5738ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-116506290.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjvelandiag\u001b[0m (\u001b[33mjulian-unal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250827_162517-wdifx1zr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/julian-unal/huggingface/runs/wdifx1zr' target=\"_blank\">earnest-serenity-50</a></strong> to <a href='https://wandb.ai/julian-unal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/julian-unal/huggingface' target=\"_blank\">https://wandb.ai/julian-unal/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/julian-unal/huggingface/runs/wdifx1zr' target=\"_blank\">https://wandb.ai/julian-unal/huggingface/runs/wdifx1zr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11564' max='11565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11564/11565 3:06:10 < 00:02, 0.36 it/s, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.841400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.834300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.860000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint guardado en paso 7750\n",
      "Checkpoint guardado en paso 8000\n",
      "Checkpoint guardado en paso 8250\n",
      "Checkpoint guardado en paso 8500\n",
      "Checkpoint guardado en paso 8750\n",
      "Checkpoint guardado en paso 9000\n",
      "Checkpoint guardado en paso 9250\n",
      "Checkpoint guardado en paso 9500\n",
      "Checkpoint guardado en paso 9750\n",
      "Checkpoint guardado en paso 10000\n",
      "Checkpoint guardado en paso 10250\n",
      "Checkpoint guardado en paso 10500\n",
      "Checkpoint guardado en paso 10750\n",
      "Checkpoint guardado en paso 11000\n",
      "Checkpoint guardado en paso 11250\n",
      "Checkpoint guardado en paso 11500\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback()]  # acción extra para avisar/gestionar cuando se guarda un checkpoint\n",
    ")\n",
    "\n",
    "print(\"Iniciando entrenamiento\")\n",
    "if last_checkpoint:\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e5a05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "de237edd39f64750a0041845ea8fc1db",
      "21dba975067246ce894a6743186352be",
      "9819bc2a5e63419a89b26853e04e2f2d",
      "543fddc819e44827ab19df93cb13e285",
      "7cc2478b57a44563a84724f97ff204c9",
      "5ecb7383074d403295147eeb68d478f7",
      "598e79ea04714ffa86a1aedf65fe9bdf",
      "f6828f61591b44398c08d42ff17aa5a7",
      "a8f162ad74cf4231863199687e61e358",
      "8ae7be3d8c0c4a17b041817d8c0d23f9",
      "6ff6117e1d7745e293acfa6ce30a3f5d",
      "2dae562788634e348ffc3cd9ee8660f2",
      "ada343e3fe8e4e9283ec7451341037aa",
      "528d181858d14c6db2143bdda842a804",
      "82f8522693514fe6859829ac100ea97c",
      "001b1f9ec77f4395b9eb0228a5b53b56",
      "c75d851c3ccb4fd7b90c7ef96e214f1b",
      "96222c3714cb45eaad5ec8bdc75790a5",
      "55d0efc03f0f40f5b26b15dd49ebf4ef",
      "301389354a7548a190c8d4c3399b9b96",
      "0156377548234955919233d0db82d2a1",
      "c7dd20fe918b49c0a7bb9ad9087533f6",
      "3b019da39585461cb96f14c1a5fd3df9",
      "f9bc3c694bb44caa9dafde623d0ee501",
      "1a87a0cb90b04bf49da715b67bebf9b9",
      "b4933fcc964f459e9749874a18dbac07",
      "fb3f42015df74d16a59e0ef72be1b820",
      "cebdc37e87de44ac83df363e64320b79",
      "c8f13576553e4abcbe4b397f31bbec1e",
      "dcea3558818645f890bdb38f49640f8e",
      "1f0399005a1e4a4c8cad9033a1ffb370",
      "136af11d34644e7c8ce67acdc20934f1",
      "c1ff2ceea6ca419db5eb64207a20f67d",
      "4b2b478488584ceab7fd82952dde0fc3",
      "88b492559abe42518576953c44e19b7c",
      "803e42e1926a401f985f7144dbbf64e4",
      "15a1d2da67a1498093fccd9f57f614c4",
      "78befb74166d4aa69732490edafce554",
      "be2bad05b54f4653bb170c3e0b485557",
      "4cef9df373254a438a1a6a5876e76ec5",
      "1fa2b9c2db3b4f298ed36dfbdd31e53b",
      "9ce278d5289e4614b74f534e2863020f",
      "8a75cb6cd34b4d8fb419e24d3e285f8d",
      "40d6c882c49b4b3f9f096f434eccb915",
      "b33722584fcc475aadcddb44a3482186",
      "e3af0472ba57490aa34125309c12b11a",
      "7f6c5d857f0543b29c419591a16f47e6",
      "9ceb7cf7568149e4b755b74cd8d75a60",
      "90f018f0ac664d26bdbca526499f79bd",
      "47430e76a89c4b2589808399fd2393c4",
      "6c3e94ece8c746519c4c1325a424a6a6",
      "5b3c64c5dbb84bb6960b2347ec15d806",
      "c7133668f033410e9ea9f94345af0b96",
      "ea3749f3e67641f6877de8aa4950e2b5",
      "816dd6f28b4d489998eea7bf6eb0ab74",
      "6383533bcbb8419683f6da5ba2b2a83e",
      "6b0954014e6142f1a44646ca9ceef8cc",
      "010e8603cf9c4e3086cc03d4bcc5db30",
      "23a631a2c5ba404cac23ef553a86d080",
      "26ad12ce0b74480ba05e9487a2a14524",
      "a3b8b3518b1f42e280558b084c210f38",
      "1e4f096e03794341a10f5f2b3ddf5c00",
      "97bba2533aed46fc94540917a9edfba3",
      "9534fdce866f421099072fcc6bac09fd",
      "8be7d0a247d34b6696e982c88437e63f",
      "c41b05c32bbf44d894ac2ad4dc21168d",
      "13610ff824ad440abd610ba850c37887",
      "71434131b6894900953a329bd758bf5e",
      "7c432f02c40e4eccbaa33e7c032f3707",
      "0627878610584fa98995ede83adbb741",
      "d24eafb19536462aa9954284dfa44814",
      "dc79b809332f4861bb34717db621cb74",
      "587400049669478fb3df6201659a3a25",
      "f76e9e7d81d2449ab3791835cfc37aaa",
      "0b28b0e7794c462c89f3ddc53e013481",
      "3d8188b1ed284ab381fe66b798a60cba",
      "f15235bd88d74f239faef594f5c3df30",
      "742543a5dc2847e0b54bbe007309f63d",
      "47a465a6831c43b1b85abe6c81ae9992",
      "b6f3d0f50b854212a96b307d8972c3c9",
      "1e9a44f8febd41ff9678c5ecf87e98ee",
      "93ea974581534192b1ebe90bb57f7b46",
      "3b18338e78314056b2bfd10dd5bc795d",
      "96d00fa461a24184b8edf50d094fe7f5",
      "9dad5ed1282640ee8d6c3567cd399c47",
      "bbacf90d0cda433d8ab2fcf5a86db885",
      "07f7824ecbda4c1b87c2334f7d1c9f7e",
      "c7f38ae2c35041af86aa9bc80ccb97a1"
     ]
    },
    "id": "ff1e5a05",
    "outputId": "5b12f554-aaeb-44c7-b5db-c75300234432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 60336, done.\u001b[K\n",
      "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
      "remote: Compressing objects: 100% (195/195), done.\u001b[K\n",
      "remote: Total 60336 (delta 158), reused 45 (delta 45), pack-reused 60096 (from 2)\u001b[K\n",
      "Receiving objects: 100% (60336/60336), 150.60 MiB | 29.88 MiB/s, done.\n",
      "Resolving deltas: 100% (43743/43743), done.\n",
      "INFO:hf-to-gguf:Loading model: merged_llama\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
      "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first %}{{ bos_token }}{% endif %}<|start_header_id|>{{ message['role'] }}<|end_header_id|>\n",
      "\n",
      "{{ message['content'] }}<|eot_id|>{% endfor %}<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/gguf_model/model-f16.gguf: n_tensors = 147, total_size = 2.5G\n",
      "Writing: 100% 2.47G/2.47G [00:40<00:00, 61.7Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/model-f16.gguf\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- ggml version: 0.0.6302\n",
      "-- ggml commit:  fbef0fad\n",
      "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
      "-- Configuring done (1.8s)\n",
      "-- Generating done (0.2s)\n",
      "-- Build files have been written to: /content/llama.cpp/build\n",
      "[  1%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  1%] \u001b[0mBuilt target build_info\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
      "[  2%] \u001b[0mBuilt target sha256\u001b[0m\n",
      "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
      "[  3%] \u001b[0mBuilt target xxhash\u001b[0m\n",
      "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
      "[  3%] \u001b[0mBuilt target sha1\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  3%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
      "[  3%] \u001b[0mBuilt target llama-llava-cli\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  5%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
      "[  5%] \u001b[0mBuilt target llama-gemma3-cli\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  6%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
      "[  6%] \u001b[0mBuilt target llama-minicpmv-cli\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  7%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
      "[  7%] \u001b[0mBuilt target llama-qwen2vl-cli\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  9%] \u001b[1m\u001b[32mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  9%] \u001b[0mBuilt target ggml-base\u001b[0m\n",
      "[  9%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[1m\u001b[32mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[ 15%] \u001b[0mBuilt target ggml-cpu\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[1m\u001b[32mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[ 15%] \u001b[0mBuilt target ggml\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
      "[ 17%] \u001b[0mBuilt target llama-gguf-hash\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
      "[ 17%] \u001b[0mBuilt target llama-gguf\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[1m\u001b[32mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 27%] \u001b[0mBuilt target llama\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[1m\u001b[32mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
      "[ 30%] \u001b[0mBuilt target mtmd\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 31%] \u001b[1m\u001b[32mLinking C executable ../bin/test-c\u001b[0m\n",
      "[ 31%] \u001b[0mBuilt target test-c\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
      "[ 32%] \u001b[0mBuilt target llama-simple\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
      "[ 33%] \u001b[0mBuilt target llama-simple-chat\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[1m\u001b[32mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 38%] \u001b[0mBuilt target common\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 39%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
      "[ 39%] \u001b[0mBuilt target test-sampling\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[0mBuilt target test-tokenizer-0\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 40%] \u001b[0mBuilt target test-grammar-parser\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 41%] \u001b[0mBuilt target test-llama-grammar\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
      "[ 43%] \u001b[0mBuilt target test-grammar-integration\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
      "[ 45%] \u001b[0mBuilt target test-json-schema-to-grammar\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-chat\u001b[0m\n",
      "[ 45%] \u001b[0mBuilt target test-chat\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
      "[ 46%] \u001b[0mBuilt target test-gbnf-validator\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
      "[ 48%] \u001b[0mBuilt target test-quantize-stats\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 49%] \u001b[0mBuilt target test-tokenizer-1-bpe\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
      "[ 50%] \u001b[0mBuilt target test-tokenizer-1-spm\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 51%] \u001b[0mBuilt target test-chat-template\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
      "[ 52%] \u001b[0mBuilt target test-json-partial\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-log\u001b[0m\n",
      "[ 53%] \u001b[0mBuilt target test-log\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
      "[ 54%] \u001b[0mBuilt target test-chat-parser\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
      "[ 56%] \u001b[0mBuilt target test-regex-partial\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
      "[ 57%] \u001b[0mBuilt target test-thread-safety\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
      "[ 59%] \u001b[0mBuilt target test-arg-parser\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-opt\u001b[0m\n",
      "[ 60%] \u001b[0mBuilt target test-opt\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
      "[ 61%] \u001b[0mBuilt target test-gguf\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 62%] \u001b[0mBuilt target test-model-load-cancel\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 63%] \u001b[0mBuilt target test-autorelease\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
      "[ 64%] \u001b[0mBuilt target test-barrier\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 65%] \u001b[0mBuilt target test-quantize-fns\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 66%] \u001b[0mBuilt target test-quantize-perf\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 67%] \u001b[0mBuilt target test-rope\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
      "[ 68%] \u001b[0mBuilt target test-mtmd-c-api\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
      "[ 69%] \u001b[0mBuilt target llama-batched\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
      "[ 70%] \u001b[0mBuilt target llama-embedding\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
      "[ 70%] \u001b[0mBuilt target llama-eval-callback\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
      "[ 71%] \u001b[0mBuilt target llama-gritlm\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
      "[ 72%] \u001b[0mBuilt target llama-lookahead\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
      "[ 73%] \u001b[0mBuilt target llama-lookup\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[1m\u001b[32mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 74%] \u001b[0mBuilt target test-backend-ops\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
      "[ 75%] \u001b[0mBuilt target llama-lookup-create\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
      "[ 76%] \u001b[0mBuilt target llama-lookup-merge\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
      "[ 76%] \u001b[0mBuilt target llama-lookup-stats\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
      "[ 77%] \u001b[0mBuilt target llama-parallel\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
      "[ 78%] \u001b[0mBuilt target llama-passkey\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
      "[ 80%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
      "[ 80%] \u001b[0mBuilt target llama-save-load-state\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[0mBuilt target llama-retrieval\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
      "[ 80%] \u001b[0mBuilt target llama-speculative-simple\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
      "[ 81%] \u001b[0mBuilt target llama-speculative\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
      "[ 83%] \u001b[0mBuilt target llama-gen-docs\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
      "[ 83%] \u001b[0mBuilt target llama-finetune\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
      "[ 84%] \u001b[0mBuilt target llama-logits\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
      "[ 86%] \u001b[0mBuilt target llama-diffusion-cli\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
      "[ 87%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
      "[ 87%] \u001b[0mBuilt target llama-convert-llama2c-to-ggml\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[0mBuilt target llama-vdot\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
      "[ 88%] \u001b[0mBuilt target llama-q8dot\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
      "[ 90%] \u001b[0mBuilt target llama-batched-bench\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
      "[ 90%] \u001b[0mBuilt target llama-gguf-split\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
      "[ 91%] \u001b[0mBuilt target llama-imatrix\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 92%] \u001b[0mBuilt target llama-bench\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "[ 92%] \u001b[0mBuilt target llama-cli\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[ 93%] \u001b[0mBuilt target llama-quantize\u001b[0m\n",
      "[ 93%] \u001b[1m\u001b[34mGenerating loading.html.hpp\u001b[0m\n",
      "[ 93%] \u001b[1m\u001b[34mGenerating index.html.gz.hpp\u001b[0m\n",
      "[ 93%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
      "[ 93%] \u001b[0mBuilt target llama-perplexity\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
      "[ 95%] \u001b[0mBuilt target llama-run\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
      "[ 96%] \u001b[0mBuilt target llama-tokenize\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
      "[ 97%] \u001b[0mBuilt target llama-tts\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
      "[ 98%] \u001b[0mBuilt target llama-mtmd-cli\u001b[0m\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
      "[ 99%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
      "[ 99%] \u001b[0mBuilt target llama-cvector-generator\u001b[0m\n",
      "[100%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[100%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "[100%] \u001b[0mBuilt target llama-export-lora\u001b[0m\n",
      "[100%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
      "[100%] \u001b[0mBuilt target llama-server\u001b[0m\n",
      "main: build = 6302 (fbef0fad)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/gguf_model/model-f16.gguf' to '/content/gguf_model/model-q4_k_m.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /content/gguf_model/model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Merged_Llama\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_sep_token bool             = false\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type  f16:  113 tensors\n",
      "[   1/ 147]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   2/ 147]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 147]                    token_embd.weight - [ 2048, 128256,     1,     1], type =    f16, converting to q6_K .. size =   501.00 MiB ->   205.49 MiB\n",
      "[   4/ 147]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[   5/ 147]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   6/ 147]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   7/ 147]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   8/ 147]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[   9/ 147]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  10/ 147]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  11/ 147]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  12/ 147]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  13/ 147]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  14/ 147]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  15/ 147]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  16/ 147]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  17/ 147]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  18/ 147]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  19/ 147]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  20/ 147]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  21/ 147]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  22/ 147]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  23/ 147]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  24/ 147]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  25/ 147]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  26/ 147]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  27/ 147]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  28/ 147]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  29/ 147]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  30/ 147]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  31/ 147]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  32/ 147]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  33/ 147]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  34/ 147]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  35/ 147]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  36/ 147]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  37/ 147]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  38/ 147]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  39/ 147]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  40/ 147]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  41/ 147]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  42/ 147]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  43/ 147]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  44/ 147]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  45/ 147]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  46/ 147]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  47/ 147]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  48/ 147]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  49/ 147]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  50/ 147]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  51/ 147]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  52/ 147]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  53/ 147]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  54/ 147]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  55/ 147]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  56/ 147]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  57/ 147]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  58/ 147]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  59/ 147]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  60/ 147]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  61/ 147]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  62/ 147]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  63/ 147]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  64/ 147]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  65/ 147]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  66/ 147]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  67/ 147]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  68/ 147]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  69/ 147]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  70/ 147]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  71/ 147]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  72/ 147]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  73/ 147]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  74/ 147]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  75/ 147]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  76/ 147]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  77/ 147]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  78/ 147]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  79/ 147]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  80/ 147]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  81/ 147]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  82/ 147]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  83/ 147]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  84/ 147]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  85/ 147]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  86/ 147]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  87/ 147]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  88/ 147]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  89/ 147]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  90/ 147]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  91/ 147]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  92/ 147]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  93/ 147]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  94/ 147]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  95/ 147]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  96/ 147]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  97/ 147]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  98/ 147]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  99/ 147]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 100/ 147]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 101/ 147]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 102/ 147]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 103/ 147]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 104/ 147]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 105/ 147]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 106/ 147]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 107/ 147]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 108/ 147]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 109/ 147]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 110/ 147]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 111/ 147]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 112/ 147]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 113/ 147]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 114/ 147]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 115/ 147]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 116/ 147]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 117/ 147]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 118/ 147]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 119/ 147]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 120/ 147]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 121/ 147]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 122/ 147]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 123/ 147]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 124/ 147]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 125/ 147]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 126/ 147]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 127/ 147]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 128/ 147]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 129/ 147]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 130/ 147]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 131/ 147]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 132/ 147]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 133/ 147]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 134/ 147]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 135/ 147]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 136/ 147]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 137/ 147]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 138/ 147]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 139/ 147]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 140/ 147]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 141/ 147]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 142/ 147]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 143/ 147]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 144/ 147]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 145/ 147]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 146/ 147]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 147/ 147]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "llama_model_quantize_impl: model size  =  2357.26 MB\n",
      "llama_model_quantize_impl: quant size  =   762.81 MB\n",
      "\n",
      "main: quantize time = 126726.24 ms\n",
      "main:    total time = 126726.24 ms\n",
      "total 3.1G\n",
      "-rw-r--r-- 1 root root 2.4G Aug 27 19:34 model-f16.gguf\n",
      "-rw-r--r-- 1 root root 771M Aug 27 19:44 model-q4_k_m.gguf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de237edd39f64750a0041845ea8fc1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae562788634e348ffc3cd9ee8660f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b019da39585461cb96f14c1a5fd3df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...tent/merged_llama/model.safetensors:   1%|1         | 25.1MB / 2.47GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2b478488584ceab7fd82952dde0fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /content/merged_llama/tokenizer.json  : 100%|##########| 17.2MB / 17.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33722584fcc475aadcddb44a3482186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6383533bcbb8419683f6da5ba2b2a83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13610ff824ad440abd610ba850c37887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /content/gguf_model/model-q4_k_m.gguf :   3%|3         | 25.0MB /  808MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742543a5dc2847e0b54bbe007309f63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /content/gguf_model/model-f16.gguf    :   1%|1         | 25.0MB / 2.48GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/JulianVelandia/Llama-3.2-1B-acento-ft-gguf/commit/6f22e03fd5648f84c87287e7338f9e4a8a1c22d8', commit_message='Upload folder using huggingface_hub', commit_description='', oid='6f22e03fd5648f84c87287e7338f9e4a8a1c22d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/JulianVelandia/Llama-3.2-1B-acento-ft-gguf', endpoint='https://huggingface.co', repo_type='model', repo_id='JulianVelandia/Llama-3.2-1B-acento-ft-gguf'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U --quiet \"huggingface_hub>=0.33.6\" cmake ninja mistral_common\n",
    "\n",
    "import os, re, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "ckpts = [d for d in os.listdir(OUTPUT_DIR) if re.match(r\"^checkpoint-\\d+$\", d)]\n",
    "assert ckpts, \"No hay checkpoints en OUTPUT_DIR\"\n",
    "last_ckpt = os.path.join(OUTPUT_DIR, max(ckpts, key=lambda x: int(x.split(\"-\")[-1])))\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"cpu\", token=HUGGINGFACE_TOKEN)\n",
    "lora_model = PeftModel.from_pretrained(base_model, last_ckpt)\n",
    "full_model = lora_model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=HUGGINGFACE_TOKEN)\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if loop.first %}{{ bos_token }}{% endif %}<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>{% endfor %}<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "merged_dir = \"/content/merged_llama\"\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "full_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "\n",
    "!rm -rf llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!mkdir -p /content/gguf_model\n",
    "!python3 llama.cpp/convert_hf_to_gguf.py /content/merged_llama --outfile /content/gguf_model/model-f16.gguf --outtype f16\n",
    "!cmake -S llama.cpp -B llama.cpp/build\n",
    "!cmake --build llama.cpp/build -j 2\n",
    "!llama.cpp/build/bin/llama-quantize /content/gguf_model/model-f16.gguf /content/gguf_model/model-q4_k_m.gguf Q4_K_M\n",
    "!ls -lh /content/gguf_model\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "api = HfApi()\n",
    "repo_id = \"JulianVelandia/Llama-3.2-1B-acento-ft-gguf\"\n",
    "api.create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
    "api.upload_folder(folder_path=merged_dir, repo_id=repo_id, repo_type=\"model\")\n",
    "api.upload_folder(folder_path=\"/content/gguf_model\", repo_id=repo_id, repo_type=\"model\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
